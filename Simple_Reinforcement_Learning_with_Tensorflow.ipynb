{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Reinforcement Learning with Tensorflow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_Reinforcement_Learning_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6jsPJyBwBX7",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Q-Learning with Tables and Neural Networks [[Link]](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyrhLtG7wFK9",
        "colab_type": "text"
      },
      "source": [
        "We begin with a simpler algorithm than we'll see in the next few tutorials. **Policy gradients** attempt to learn functions which map observations to actions, whereas **Q-Learning** attempts to learn the value of being in a given state, and taking an action there. Even DeepQ networks are just larger and more complex versions of the algorithm we'll discuss here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtylpYywreN",
        "colab_type": "text"
      },
      "source": [
        "We'll be working in the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment from OpenAI Gym. This environment is a 4x4 grid of blocks, with blocks being one of the start block, goal block, a safe block, or dangerous hole. We want our agent to navigate to the goal without falling down a hole, but the ice is slippery and we might not move exactly as the agent attempts to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K4bkqAuyGUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d31177a9-53f5-41f6-9bc3-2c2ffd1367a2"
      },
      "source": [
        "# Import Gym, create and view environment\n",
        "import gym\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.render()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QpbmQY8zhgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1bf1408f-e2fe-4c97-b47c-cd140dbf40f7"
      },
      "source": [
        "# View actions/observations\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n",
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYYoefwSzkCb",
        "colab_type": "text"
      },
      "source": [
        "Discrete spaces allow a fixed range of non-negative numbers. Thus we have 4 possible actions, 0 to 3, and 16 possible observations for which of the 16 squares our agent is on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbwswcGw2Ba0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "14a69ea9-6c31-440f-ab02-fe03a90b4653"
      },
      "source": [
        "# Note: The environment isn't randomized.\n",
        "# It's the same each time.\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWwFV0231xPh",
        "colab_type": "text"
      },
      "source": [
        "Our reward at every step is 0, except when we enter the goal, where we receive a reward of 1. This means we need an algorithm that can learn with long-term expected rewards, which Q-Learning can. The simplest implementation of Q-Learning is a table of values for each state (row) and action (column) possible. We learn a value for how good it is to take a given action within a given state for each combination. First, initialize all $16\\times4=64$ cells to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtIzyvbO3I3F",
        "colab_type": "text"
      },
      "source": [
        "To update our table, we'll use the Bellman equation. This states that \"the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.\" Put differently (in my words), we want to figure out how valuable some choice is in the future. We can break this question into two parts:\n",
        "1. How valuable is this decision immediately? This is easy to answer, because we immediately see the next step following our decision.\n",
        "2. How valuable is the best choice we can then make? We've now recursively broken the question down into a simpler problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZSPXffF4bW0",
        "colab_type": "text"
      },
      "source": [
        "This approach allows us to reuse the Q-table when estimating how to update the table for some current decision. Formally, we could write:\n",
        "\n",
        "$Q(s,a)=r + \\gamma(\\text{max}\\{Q(s',a')\\})$, where the Q-value for some given state $s$ and action $a$ is the current reward $r$ plus the maximum discounted reward $\\gamma$ according the the next state $s'$ our decision results in. The discount function $\\gamma$ allows us to vary how important possible future rewards are compared to the present reward. Thus we slowly develop an accurate table of expected rewards for given actions in given states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHM-9oz061ag",
        "colab_type": "text"
      },
      "source": [
        "From [A Beginner's Guide to Deep Reinforcement Learning](https://skymind.ai/wiki/deep-reinforcement-learning#define):\n",
        "\n",
        "We multiply future rewards by a **discount factor** in order to lessen the impact they have on present decisions. This makes future rewards worth less than present rewards, but the gamma $\\gamma$ parameter lets us pick exactly how much. As an example, with $\\gamma=0.8$, and a reward of 10 in 3 time steps, the present value of the reward is $0.8^3*10$. A gamma of 1 thus treats future rewards as just as valuable as immediate rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrX-6NnX7nKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def discount(arr, gamma):\n",
        "  \"\"\" Discounts a given list of rewards using the given gamma.\n",
        "      Gives the same results as an example from a workshop, so I trust my code here. \"\"\"\n",
        "  \n",
        "  for r in range(len(arr)): # Loop over all rewards\n",
        "    for i in range(r + 1, len(arr)): # For all future rewards...\n",
        "      arr[r] += gamma ** (i-r) * arr[i] # Add discounted future rewards\n",
        "    \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WElBBIKJ72Mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "64411602-de17-4632-9e9a-37ecab4a9a2a"
      },
      "source": [
        "discount([1,2,3,1,2,3,50], 0.56)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.140374425600002,\n",
              " 7.393525760000001,\n",
              " 9.631296000000003,\n",
              " 11.841600000000003,\n",
              " 19.360000000000003,\n",
              " 31.000000000000004,\n",
              " 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfeP2kKM6C8N",
        "colab_type": "text"
      },
      "source": [
        "At this point, I'll attempt to code this out myself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g9s6QI0fVnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Q_Table_Agent:\n",
        "\n",
        "    # Initializer\n",
        "    def __init__(self, states, actions, gamma, learning_rate):\n",
        "      self.Q_t = np.zeros((states, actions)) # Table dimension is: states by possible actions.\n",
        "      self.g = gamma # The discount factor for this agent.\n",
        "      self.lr = learning_rate # The learning rate for our Q-table updates.\n",
        "      \n",
        "    def max_col(self, row):\n",
        "      \"\"\" Returns one of the columns with the max value in a given numpy row. \"\"\"\n",
        "      maximum = np.amax(row) # Find the maximum value in the row.\n",
        "      winners = []\n",
        "  \n",
        "      for c in range(len(row)): # Loop over all values in the row.\n",
        "        if (row[c] == maximum):\n",
        "          winners.append(c) # Append columns containing the maximum value to list.\n",
        "  \n",
        "      return random.sample(winners, 1)[0] # Return a random one of these columns.\n",
        "        \n",
        "    def act(self, state):\n",
        "      \"\"\" Chooses an action given some state. \"\"\"\n",
        "      row = self.Q_t[state] # Choose the maximum column for this state.\n",
        "      return self.max_col(row) # This will be the action yielding the highest expected reward.\n",
        "    \n",
        "    def setTable(self, r, c, value):\n",
        "      \"\"\" Sets Q_t[r][c] to the given value. \"\"\"\n",
        "      self.Q_t[r][c] = value\n",
        "      return\n",
        "    \n",
        "    def learn(self, s, a, new_s, reward):\n",
        "      \"\"\" Learn based on action <a> we took in given state <s>. \"\"\"\n",
        "      \n",
        "      max_exp = np.amax(self.Q_t[new_s]) # Find the maximum reward we can expect in the future.\n",
        "      # print(\"Maximum expected from new state\", max_exp)\n",
        "      \n",
        "      set_to = reward + self.g * max_exp # If our learning rate was 1, we'd set Q_t[s][a] to this.\n",
        "      # print(\"Would set value to:\", set_to)\n",
        "  \n",
        "      delta = self.lr * (set_to - self.Q_t[s][a]) # Change the reward by this much.\n",
        "      # print(\"Q_t[s][a] is\", self.Q_t[s][a])\n",
        "      # print(\"Should change by:\", delta)\n",
        "  \n",
        "      self.Q_t[s][a] += delta\n",
        "  \n",
        "      # print(\"Q_t[s][a] is now:\", self.Q_t[s][a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VfyJE_6lpIS",
        "colab": {}
      },
      "source": [
        "# Create an agent for a few tests.\n",
        "\n",
        "agent = Q_Table_Agent(env.observation_space.n, env.action_space.n, 0.95, 0.8)\n",
        "agent.setTable(0,0,1)\n",
        "agent.setTable(0,1,2)\n",
        "agent.setTable(1,0,3)\n",
        "agent.setTable(1,2,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQEg3sy-iXsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9dfb384-3b82-427b-862a-60e217b562c4"
      },
      "source": [
        "# Pretend we're in this state, which action should we take? Seems to work.\n",
        "agent.act(1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpIh_v5MixU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "87516118-7f1a-4ad1-e0d6-e38557180b9b"
      },
      "source": [
        "# Seems to work now.\n",
        "agent.learn(0,1,1,1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum expected from new state 3.0\n",
            "Would set value to: 3.8499999999999996\n",
            "Q_t[s][a] is 2.0\n",
            "Should change by: 1.4799999999999998\n",
            "Q_t[s][a] is now: 3.4799999999999995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImff4yn6BqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q-Table Implementation on OpenAI Gym FrozenLake\n",
        "\n",
        "episodes = 20000 # Number of episodes to train for.\n",
        "timesteps = 500 # Cut the agent off after this many timesteps in any one episode.\n",
        "\n",
        "agent = Q_Table_Agent(env.observation_space.n, env.action_space.n, 0.9, 0.8)\n",
        "\n",
        "records = [] # To store our scores over all episodes.\n",
        "\n",
        "for ep in range(episodes):\n",
        "  state = env.reset() # Gets the state we start in, which is 0.\n",
        "  \n",
        "  for t in range(timesteps):\n",
        "    action = agent.act(state)\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action) # Try our action in the environment.\n",
        "    \n",
        "    agent.learn(state, action, new_state, reward) # Learn from what we did.\n",
        "    \n",
        "    state = new_state\n",
        "    \n",
        "    if done:\n",
        "      records.append(reward) # Record if we won.\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdAV72ULQLOO",
        "colab_type": "text"
      },
      "source": [
        "### Debugging time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-CLwrbjeSZc",
        "colab_type": "text"
      },
      "source": [
        "Recall, we want: $Q(s,a)=r + \\gamma(\\text{max}\\{Q(s',a')\\})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2isNy3gNG-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "aa14ee8f-9522-4b4c-a656-64f0dc56cd67"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(records)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd6fc0a1d30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6QOMtwlqYMMd0nqIMNdkjrIcJekDvo/hD8+kCyDRK8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1y2P8lTnX-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ade7c4f6-2487-453b-bce3-3fd20a09f114"
      },
      "source": [
        "sum(records)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4989.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWLtSFxKmsQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d178cc46-5400-489e-d9c9-221db75f9902"
      },
      "source": [
        "print(agent.Q_t)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 2.23368744e-02 0.00000000e+00 0.00000000e+00]\n",
            " [1.63119652e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 1.10312259e-04 0.00000000e+00]\n",
            " [2.11096791e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 4.55247503e-03 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.21043875e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 2.28735398e-02 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.54749655e-02]\n",
            " [0.00000000e+00 0.00000000e+00 6.79742905e-03 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 9.97855663e-01 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a0iNjzMBgs8",
        "colab_type": "text"
      },
      "source": [
        "These were helpful:\n",
        "* Gym Discrete Space code [here](https://github.com/openai/gym/blob/master/gym/spaces/discrete.py)\n",
        "* For [sampling a list](https://www.geeksforgeeks.org/python-random-sample-function/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zVsxVvr0PnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Close the FrozenLake environment.\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
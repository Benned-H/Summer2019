{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Reinforcement Learning with Tensorflow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_Reinforcement_Learning_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6jsPJyBwBX7",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Q-Learning with Tables and Neural Networks [[Link]](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyrhLtG7wFK9",
        "colab_type": "text"
      },
      "source": [
        "We begin with a simpler algorithm than we'll see in the next few tutorials. **Policy gradients** attempt to learn functions which map observations to actions, whereas **Q-Learning** attempts to learn the value of being in a given state, and taking an action there. Even DeepQ networks are just larger and more complex versions of the algorithm we'll discuss here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtylpYywreN",
        "colab_type": "text"
      },
      "source": [
        "We'll be working in the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment from OpenAI Gym. This environment is a 4x4 grid of blocks, with blocks being one of the start block, goal block, a safe block, or dangerous hole. We want our agent to navigate to the goal without falling down a hole, but the ice is slippery and we might not move exactly as the agent attempts to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K4bkqAuyGUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "92eec623-6310-4f4c-a438-b09960b3fdde"
      },
      "source": [
        "# Import Gym, create and view environment\n",
        "import gym\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.render()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QpbmQY8zhgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63d2b416-44be-4d8c-a215-64acb58741a2"
      },
      "source": [
        "# View actions/observations\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n",
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYYoefwSzkCb",
        "colab_type": "text"
      },
      "source": [
        "Discrete spaces allow a fixed range of non-negative numbers. Thus we have 4 possible actions, 0 to 3, and 16 possible observations for which of the 16 squares our agent is on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbwswcGw2Ba0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "107a6d43-7878-4e4f-cf09-498709ef0639"
      },
      "source": [
        "# Note: The environment isn't randomized.\n",
        "# It's the same each time.\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWwFV0231xPh",
        "colab_type": "text"
      },
      "source": [
        "Our reward at every step is 0, except when we enter the goal, where we receive a reward of 1. This means we need an algorithm that can learn with long-term expected rewards, which Q-Learning can. The simplest implementation of Q-Learning is a table of values for each state (row) and action (column) possible. We learn a value for how good it is to take a given action within a given state for each combination. First, initialize all $16\\times4=64$ cells to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtIzyvbO3I3F",
        "colab_type": "text"
      },
      "source": [
        "To update our table, we'll use the Bellman equation. This states that \"the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.\" Put differently (in my words), we want to figure out how valuable some choice is in the future. We can break this question into two parts:\n",
        "1. How valuable is this decision immediately? This is easy to answer, because we immediately see the next step following our decision.\n",
        "2. How valuable is the best choice we can then make? We've now recursively broken the question down into a simpler problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZSPXffF4bW0",
        "colab_type": "text"
      },
      "source": [
        "This approach allows us to reuse the Q-table when estimating how to update the table for some current decision. Formally, we could write:\n",
        "\n",
        "$Q(s,a)=r + \\gamma(\\text{max}\\{Q(s',a')\\})$, where the Q-value for some given state $s$ and action $a$ is the current reward $r$ plus the maximum discounted reward $\\gamma$ according the the next state $s'$ our decision results in. The discount function $\\gamma$ allows us to vary how important possible future rewards are compared to the present reward. Thus we slowly develop an accurate table of expected rewards for given actions in given states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHM-9oz061ag",
        "colab_type": "text"
      },
      "source": [
        "From [A Beginner's Guide to Deep Reinforcement Learning](https://skymind.ai/wiki/deep-reinforcement-learning#define):\n",
        "\n",
        "We multiply future rewards by a **discount factor** in order to lessen the impact they have on present decisions. This makes future rewards worth less than present rewards, but the gamma $\\gamma$ parameter lets us pick exactly how much. As an example, with $\\gamma=0.8$, and a reward of 10 in 3 time steps, the present value of the reward is $0.8^3*10$. A gamma of 1 thus treats future rewards as just as valuable as immediate rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrX-6NnX7nKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def discount(arr, gamma):\n",
        "  \"\"\" Discounts a given list of rewards using the given gamma.\n",
        "      Gives the same results as an example from a workshop, so I trust my code here. \"\"\"\n",
        "  \n",
        "  for r in range(len(arr)): # Loop over all rewards\n",
        "    for i in range(r + 1, len(arr)): # For all future rewards...\n",
        "      arr[r] += gamma ** (i-r) * arr[i] # Add discounted future rewards\n",
        "    \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WElBBIKJ72Mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "dabc1078-d7f3-449d-a7f9-57d860043375"
      },
      "source": [
        "discount([1,2,3,1,2,3,50], 0.56)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.140374425600002,\n",
              " 7.393525760000001,\n",
              " 9.631296000000003,\n",
              " 11.841600000000003,\n",
              " 19.360000000000003,\n",
              " 31.000000000000004,\n",
              " 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfeP2kKM6C8N",
        "colab_type": "text"
      },
      "source": [
        "At this point, I'll attempt to code this out myself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SVroeZIFyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_col_in_row(row):\n",
        "  \"\"\" Returns one of the columns with the max value in a given numpy row. \"\"\"\n",
        "  maximum = np.amax(row) # Find the maximum value in the row.\n",
        "  winners = []\n",
        "  \n",
        "  for c in range(len(row)):\n",
        "    if (row[c] == maximum):\n",
        "      winners.append(c) # Create list of columns containing the maximum value.\n",
        "  \n",
        "  return random.sample(winners, 1)[0]\n",
        "\n",
        "def act(Q_table, obs):\n",
        "  \"\"\" Chooses an action given an observation and our Q-table. \"\"\"\n",
        "  row = Q_table[obs] # Choose the maximum column for this state\n",
        "  return max_col_in_row(row) # This will be the action yielding the highest expected reward.\n",
        "\n",
        "def learn(Q_t, s, a, new_obs, r, lr, g):\n",
        "  \"\"\" Learn based on action <a> we took in given state <s>. \"\"\"\n",
        "  max_exp = np.amax(Q_t[new_obs]) # Find the maximum reward we can expect in the future.\n",
        "  delta = r + (g * max_exp) - Q_t[s][a] # Change the reward by this much.\n",
        "  Q_t[s][a] += lr * delta\n",
        "  \n",
        "  return Q_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImff4yn6BqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q-Table Implementation on OpenAI Gym FrozenLake\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "episodes = 2000 # Number of episodes to train for.\n",
        "timesteps = 1000 # Cut the agent off after this many timesteps.\n",
        "gamma = 0.95\n",
        "learning_rate = 0.8 # Avoid overcommitting to any one lesson.\n",
        "\n",
        "Q_table = np.zeros((env.observation_space.n, env.action_space.n)) # States by possible actions, 16 x 4.\n",
        "\n",
        "records = [] # To store our scores over all episodes.\n",
        "\n",
        "for ep in range(episodes):\n",
        "  obs = env.reset() # Gets the current state we start in, which is 0.\n",
        "  \n",
        "  for t in range(timesteps):\n",
        "    action = act(Q_table, obs) # Get our action.\n",
        "\n",
        "    new_obs, reward, done, info = env.step(action) # Try our action in the environment.\n",
        "    \n",
        "    Q_table = learn(Q_table, obs, action, new_obs, reward, learning_rate, gamma) # Learn from what we did.\n",
        "    \n",
        "    obs = new_obs\n",
        "    \n",
        "    if done:\n",
        "      records.append(reward) # Record if we won.\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdAV72ULQLOO",
        "colab_type": "text"
      },
      "source": [
        "### Debugging time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX2TlnpKPn-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "dcd0e897-8dfa-4d8e-a0fd-03f9c601b830"
      },
      "source": [
        "Q_table = np.zeros((env.observation_space.n, env.action_space.n)) # States by possible actions, 16 x 4.\n",
        "Q_table[0][0] = 1\n",
        "Q_table[0][1] = 2\n",
        "Q_table[1][0] = 3\n",
        "Q_table[1][2] = 3\n",
        "print(Q_table)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 0. 0.]\n",
            " [3. 0. 3. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMTsH-Q9P4P7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "162e7d9b-0310-4f5f-dc0d-a14a8e87221b"
      },
      "source": [
        "# The act method seems to work as intended.\n",
        "act(Q_table, 1)"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2isNy3gNG-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5d0f25ce-b1c0-4f6e-83be-c7132b5626ec"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(records)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f359c6026a0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZRCDOxFRBjG4ExFlEIM7EVEG/X84qXtKQlWW8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2AUP00mIthw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1107ad39-5cf9-43a3-c641-cbe8c4f7c8b7"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a0iNjzMBgs8",
        "colab_type": "text"
      },
      "source": [
        "These were helpful:\n",
        "* Gym Discrete Space code [here](https://github.com/openai/gym/blob/master/gym/spaces/discrete.py)\n",
        "* For [sampling a list](https://www.geeksforgeeks.org/python-random-sample-function/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QiUT-Y-BK1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69794cdc-8683-431e-e249-98d795adaa27"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zVsxVvr0PnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Close the FrozenLake environment.\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
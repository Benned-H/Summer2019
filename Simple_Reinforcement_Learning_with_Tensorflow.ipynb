{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Reinforcement Learning with Tensorflow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_Reinforcement_Learning_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6jsPJyBwBX7",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Q-Learning with Tables and Neural Networks [[Link]](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyrhLtG7wFK9",
        "colab_type": "text"
      },
      "source": [
        "We begin with a simpler algorithm than we'll see in the next few tutorials. **Policy gradients** attempt to learn functions which map observations to actions, whereas **Q-Learning** attempts to learn the value of being in a given state, and taking an action there. Even DeepQ networks are just larger and more complex versions of the algorithm we'll discuss here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtylpYywreN",
        "colab_type": "text"
      },
      "source": [
        "We'll be working in the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment from OpenAI Gym. This environment is a 4x4 grid of blocks, with blocks being one of the start block, goal block, a safe block, or dangerous hole. We want our agent to navigate to the goal without falling down a hole, but the ice is slippery and we might not move exactly as the agent attempts to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K4bkqAuyGUd",
        "colab_type": "code",
        "outputId": "be70781b-453d-4e33-831e-e7e11b88a3f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Import Gym, create and view environment\n",
        "import gym\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.render()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QpbmQY8zhgn",
        "colab_type": "code",
        "outputId": "1bd4859a-51c2-4ad6-b11e-920016c29c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# View actions/observations\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n",
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYYoefwSzkCb",
        "colab_type": "text"
      },
      "source": [
        "Discrete spaces allow a fixed range of non-negative numbers. Thus we have 4 possible actions, 0 to 3, and 16 possible observations for which of the 16 squares our agent is on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbwswcGw2Ba0",
        "colab_type": "code",
        "outputId": "8febb9f0-4250-47a5-efbc-8bcdbb3975d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Note: The environment isn't randomized.\n",
        "# It's the same each time.\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWwFV0231xPh",
        "colab_type": "text"
      },
      "source": [
        "Our reward at every step is 0, except when we enter the goal, where we receive a reward of 1. This means we need an algorithm that can learn with long-term expected rewards, which Q-Learning can. The simplest implementation of Q-Learning is a table of values for each state (row) and action (column) possible. We learn a value for how good it is to take a given action within a given state for each combination. First, initialize all $16\\times4=64$ cells to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtIzyvbO3I3F",
        "colab_type": "text"
      },
      "source": [
        "To update our table, we'll use the Bellman equation. This states that \"the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.\" Put differently (in my words), we want to figure out how valuable some choice is in the future. We can break this question into two parts:\n",
        "1. How valuable is this decision immediately? This is easy to answer, because we immediately see the next step following our decision.\n",
        "2. How valuable is the best choice we can then make? We've now recursively broken the question down into a simpler problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZSPXffF4bW0",
        "colab_type": "text"
      },
      "source": [
        "This approach allows us to reuse the Q-table when estimating how to update the table for some current decision. Formally, we could write:\n",
        "\n",
        "$Q(s,a)=r + \\gamma(\\text{max}\\{Q(s',a')\\})$, where the Q-value for some given state $s$ and action $a$ is the current reward $r$ plus the maximum discounted reward $\\gamma$ according the the next state $s'$ our decision results in. The discount function $\\gamma$ allows us to vary how important possible future rewards are compared to the present reward. Thus we slowly develop an accurate table of expected rewards for given actions in given states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHM-9oz061ag",
        "colab_type": "text"
      },
      "source": [
        "From [A Beginner's Guide to Deep Reinforcement Learning](https://skymind.ai/wiki/deep-reinforcement-learning#define):\n",
        "\n",
        "We multiply future rewards by a **discount factor** in order to lessen the impact they have on present decisions. This makes future rewards worth less than present rewards, but the gamma $\\gamma$ parameter lets us pick exactly how much. As an example, with $\\gamma=0.8$, and a reward of 10 in 3 time steps, the present value of the reward is $0.8^3*10$. A gamma of 1 thus treats future rewards as just as valuable as immediate rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrX-6NnX7nKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def discount(arr, gamma):\n",
        "  \"\"\" Discounts a given list of rewards using the given gamma.\n",
        "      Gives the same results as an example from a workshop, so I trust my code here. \"\"\"\n",
        "  \n",
        "  for r in range(len(arr)): # Loop over all rewards\n",
        "    for i in range(r + 1, len(arr)): # For all future rewards...\n",
        "      arr[r] += gamma ** (i-r) * arr[i] # Add discounted future rewards\n",
        "    \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WElBBIKJ72Mo",
        "colab_type": "code",
        "outputId": "f5865e35-a4f9-4eed-f09e-6a87e2b3e71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "discount([1,2,3,1,2,3,50], 0.56)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.140374425600002,\n",
              " 7.393525760000001,\n",
              " 9.631296000000003,\n",
              " 11.841600000000003,\n",
              " 19.360000000000003,\n",
              " 31.000000000000004,\n",
              " 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfeP2kKM6C8N",
        "colab_type": "text"
      },
      "source": [
        "At this point, I'll attempt to code this out myself. These were helpful:\n",
        "* Gym Discrete Space code [here](https://github.com/openai/gym/blob/master/gym/spaces/discrete.py)\n",
        "* For [sampling a list](https://www.geeksforgeeks.org/python-random-sample-function/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g9s6QI0fVnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Q_Table_Agent:\n",
        "\n",
        "    # Initializer\n",
        "    def __init__(self, states, actions, gamma, learning_rate):\n",
        "      self.Q_t = np.zeros((states, actions)) # Table dimension is: states by possible actions.\n",
        "      self.g = gamma # The discount factor for this agent.\n",
        "      self.lr = learning_rate # The learning rate for our Q-table updates.\n",
        "      \n",
        "    def max_col(self, row):\n",
        "      \"\"\" Returns one of the columns with the max value in a given numpy row. \"\"\"\n",
        "      maximum = np.amax(row) # Find the maximum value in the row.\n",
        "      winners = []\n",
        "  \n",
        "      for c in range(len(row)): # Loop over all values in the row.\n",
        "        if (row[c] == maximum):\n",
        "          winners.append(c) # Append columns containing the maximum value to list.\n",
        "  \n",
        "      return random.sample(winners, 1)[0] # Return a random one of these columns.\n",
        "        \n",
        "    def act(self, state):\n",
        "      \"\"\" Chooses an action given some state. \"\"\"\n",
        "      row = self.Q_t[state] # Choose the maximum column for this state.\n",
        "      return self.max_col(row) # This will be the action yielding the highest expected reward.\n",
        "    \n",
        "    def setTable(self, r, c, value):\n",
        "      \"\"\" Sets Q_t[r][c] to the given value. \"\"\"\n",
        "      self.Q_t[r][c] = value\n",
        "    \n",
        "    def learn(self, s, a, new_s, reward):\n",
        "      \"\"\" Learn based on action <a> we took in given state <s>. \"\"\"\n",
        "      \n",
        "      max_exp = np.amax(self.Q_t[new_s]) # Find the maximum reward we can expect in the future.\n",
        "      set_to = reward + self.g * max_exp # If our learning rate was 1, we'd set Q_t[s][a] to this.\n",
        "      delta = self.lr * (set_to - self.Q_t[s][a]) # Change the reward by this much.\n",
        "      \n",
        "      self.Q_t[s][a] += delta\n",
        "      \n",
        "    def reset(self):\n",
        "      \"\"\"Resets the agent as if it was just created.\"\"\"\n",
        "      self.Q_t = np.zeros((len(self.Q_t), len(self.Q_t[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VfyJE_6lpIS",
        "colab": {}
      },
      "source": [
        "# Create an agent for a few tests.\n",
        "\n",
        "agent = Q_Table_Agent(env.observation_space.n, env.action_space.n, 0.95, 0.8)\n",
        "agent.setTable(0,0,1)\n",
        "agent.setTable(0,1,2)\n",
        "agent.setTable(1,0,3)\n",
        "agent.setTable(1,2,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQEg3sy-iXsO",
        "colab_type": "code",
        "outputId": "c339fbdd-a8d8-4c2d-b7f1-f59a2448c43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Pretend we're in this state, which action should we take? Seems to work.\n",
        "agent.act(1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImff4yn6BqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q-Table Implementation on OpenAI Gym FrozenLake\n",
        "\n",
        "episodes = 2000 # Number of episodes to train for.\n",
        "timesteps = 500 # Cut the agent off after this many timesteps in any one episode.\n",
        "\n",
        "agent = Q_Table_Agent(env.observation_space.n, env.action_space.n, 0.9, 0.8)\n",
        "\n",
        "records = [] # To store our scores over all episodes.\n",
        "\n",
        "for ep in range(episodes):\n",
        "  state = env.reset() # Gets the state we start in, which is 0.\n",
        "  \n",
        "  for t in range(timesteps):\n",
        "    action = agent.act(state)\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action) # Try our action in the environment.\n",
        "    \n",
        "    if (done and reward == 0):\n",
        "      agent.learn(state, action, new_state, -1) # Death case.\n",
        "    else:\n",
        "      agent.learn(state, action, new_state, reward) # Learn from what we did.\n",
        "    \n",
        "    state = new_state\n",
        "    \n",
        "    if done:\n",
        "      records.append(reward) # Record if we won.\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2isNy3gNG-p",
        "colab_type": "code",
        "outputId": "9f09639f-c32c-48d7-8cd8-ec88a9a371f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(records)\n",
        "\n",
        "# We succeeded this % of the time.\n",
        "sum(records)/episodes"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWLtSFxKmsQJ",
        "colab_type": "code",
        "outputId": "64414052-9e79-4a28-deea-fd725d6016d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(agent.Q_t)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.59400663 -0.89133055 -0.89565769 -0.89434908]\n",
            " [-0.98377372 -0.99985577 -0.99992534 -1.10878463]\n",
            " [-1.33617503 -1.01649646 -0.23933175 -0.98423717]\n",
            " [-0.98944    -0.98716147 -0.97665723 -0.09681584]\n",
            " [-0.840275   -0.95408504 -0.96       -0.89841121]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.99997764 -0.99999161 -0.99406814 -0.99998464]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.99732671 -0.98218179 -0.9977372  -0.99601884]\n",
            " [-0.99937871 -1.12948531 -0.24391722 -0.99912146]\n",
            " [-0.11355975 -0.94933234 -0.96       -0.89720792]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.96       -0.935424    0.57002357 -0.85703964]\n",
            " [-0.89510277 -0.61039844 -0.53253066  0.96259454]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7S7LHwY2p6u",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the agent is pretty terrible. The article tangentially mentions $\\epsilon$-greedy agents, so I'd like to look into that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRsQLJu2-YD",
        "colab_type": "text"
      },
      "source": [
        "## The Epsilon-Greedy Algorithm [[Link]](https://jamesmccaffrey.wordpress.com/2017/11/30/the-epsilon-greedy-algorithm/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTBMQtiR3JAY",
        "colab_type": "text"
      },
      "source": [
        "This simple algorithm is used in several areas of ML. A common use is the multi-armed bandit problem: Say we have $k=3$ slot machines. Each pays out with a different distribution, and we can only play 100 times. We want to simultaneously:\n",
        "1. Experiment with each machine to get an idea of which pays out the most.\n",
        "2. Get as much money as possible.\n",
        "\n",
        "These options, often called \"explore\" and \"exploit,\" represent the fundamental tradeoff we wish to solve. Epsilon-greedy does this extremely simply:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GiCp5eI5tHJ",
        "colab_type": "text"
      },
      "source": [
        "As we play, we track the average payout of each machine. We default to always selecting the machine with the highest current average payout. Let $\\epsilon=$ some small value, say 0.1. We'll select this machine with probability $(1-\\epsilon) + (\\epsilon-k)$, and all other machines with probability $(\\epsilon-k)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc_7NvS27Wrm",
        "colab_type": "text"
      },
      "source": [
        "More conceretely, on each attempt:\n",
        "1. Generate a random number  $p$ between 0 and 1.\n",
        "2. If $p>\\epsilon$, select the highest-paying machine. If not, select a random machine.\n",
        "\n",
        "Over time, we'll tend towards the highest-paying machine while still randomly gathering data on the other machines. As a final summary, we greedily pick the best current option most of the time, but pick a random option with small $\\epsilon$ probability. That's it.\n",
        "\n",
        "*--Fin 5/28/2019--*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpQUgXLi7efn",
        "colab_type": "text"
      },
      "source": [
        "Now to add that into our code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDpuOoFN7gYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Q_Table_Agent_ep(Q_Table_Agent):\n",
        "\n",
        "    # Initializer, overrides parent's init.\n",
        "    def __init__(self, states, actions, gamma, learning_rate, epsilon):\n",
        "      self.Q_t = np.zeros((states, actions)) # Table dimension is: states by possible actions.\n",
        "      self.g = gamma # The discount factor for this agent.\n",
        "      self.lr = learning_rate # The learning rate for our Q-table updates.\n",
        "      self.ep = epsilon # For epsilon-greedy searching of options.\n",
        "      \n",
        "    def act(self, state):\n",
        "      \"\"\" Chooses an action given some state. \"\"\"\n",
        "      p = random.random() # Generate random number 0 to 1.\n",
        "      \n",
        "      if p < self.ep: # Explore.\n",
        "        return random.randint(0, self.Q_t.shape[1]-1)\n",
        "      \n",
        "      # Else case, exploit.\n",
        "      row = self.Q_t[state] # Choose the maximum column for this state.\n",
        "      return self.max_col(row) # This will be the action yielding the highest expected reward."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yt4aJpE9Qpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Epsilon-greedy Q-Table Implementation on OpenAI Gym FrozenLake\n",
        "\n",
        "episodes = 2000 # Number of episodes to train for.\n",
        "timesteps = 500 # Cut the agent off after this many timesteps in any one episode.\n",
        "\n",
        "ep_agent = Q_Table_Agent_ep(env.observation_space.n, env.action_space.n, 0.9, 0.8, 0.01)\n",
        "\n",
        "ep_records = [] # To store our scores over all episodes.\n",
        "\n",
        "for ep in range(episodes):\n",
        "  state = env.reset() # Gets the state we start in, which is 0.\n",
        "  \n",
        "  for t in range(timesteps):\n",
        "    action = ep_agent.act(state)\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action) # Try our action in the environment.\n",
        "    \n",
        "    if (done and reward == 0):\n",
        "      ep_agent.learn(state, action, new_state, -1) # Death case.\n",
        "    else:\n",
        "      ep_agent.learn(state, action, new_state, reward) # Learn from what we did.\n",
        "    \n",
        "    state = new_state\n",
        "    \n",
        "    if done:\n",
        "      ep_records.append(reward) # Record if we won.\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6-82zKn9xQB",
        "colab_type": "code",
        "outputId": "e69fff97-232b-4c89-beef-68c3a31b4a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.plot(ep_records)\n",
        "\n",
        "# We succeeded this % of the time.\n",
        "sum(ep_records)/episodes"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3605"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO4GaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRKiSFLH98Ag",
        "colab_type": "code",
        "outputId": "9d842f9c-6234-41d1-c84b-9de2aca4ae11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(ep_agent.Q_t)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.19715727 -0.54285415 -0.04843547 -0.16943268]\n",
            " [-0.9720719  -0.92833557 -0.96900057 -0.1184625 ]\n",
            " [-0.38136943 -0.17165466 -0.85307302 -0.80299819]\n",
            " [-0.84018832 -0.37766928 -0.86347835 -0.04626952]\n",
            " [-0.099901   -0.99314439 -0.85790868 -0.2868971 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.68862593 -0.99999858 -0.99986174 -0.999237  ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.79605784 -0.94818437 -0.75003461 -0.24679802]\n",
            " [-0.94072281 -0.39642032 -0.93275263 -0.89456228]\n",
            " [-1.3377369  -0.99796627 -0.99161519 -0.97897071]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.79922216 -0.94278081 -0.00265286 -0.98999812]\n",
            " [-0.50154901  0.93482316 -0.42247918 -0.510846  ]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zVsxVvr0PnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Close the previous FrozenLake environment.\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nai0GITTpoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "class EnvRunner:\n",
        "    # Initializer\n",
        "    def __init__(self, env_name):\n",
        "      self.env = gym.make(env_name) # Create environment of given type.\n",
        "      \n",
        "    def runEnv(self, ag, episodes, timesteps):\n",
        "      \"\"\"Trains the given agent with given conditions.\n",
        "         Returns the average reward over these episodes.\"\"\"\n",
        "      \n",
        "      agent = copy.deepcopy(ag)\n",
        "      \n",
        "      scores = [] # To store our scores over all episodes.\n",
        "      \n",
        "      for ep in range(episodes):\n",
        "        state = self.env.reset()\n",
        "  \n",
        "        for t in range(timesteps):\n",
        "          action = agent.act(state)\n",
        "    \n",
        "          # Try our action in the environment.\n",
        "          new_state, reward, done, info = self.env.step(action)\n",
        "          \n",
        "          # Learn from what we did.\n",
        "          agent.learn(state, action, new_state, reward)\n",
        "    \n",
        "          state = new_state\n",
        "    \n",
        "          if done:\n",
        "            scores.append(reward) # Record if we won.\n",
        "            break\n",
        "            \n",
        "      return sum(scores) / episodes\n",
        "    \n",
        "    def getEnvParams(self):\n",
        "      return self.env.observation_space.n, self.env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyMq3d8iyCoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridSearch:\n",
        "  # Tests an agent of a given type with varying hyperparameters.\n",
        "  \n",
        "  def __init__(self, runner, eps):\n",
        "    self.runner = runner\n",
        "    self.episodes = eps\n",
        "    \n",
        "  def search(self, agent, attrs, vals, keep_num):\n",
        "    \"\"\"Searches over all combinations of the given attribute values.\"\"\"\n",
        "    if len(attrs) != len(vals):\n",
        "      print(\"ERROR:\", len(attrs), \"attributes and\", len(vals), \"values given.\")\n",
        "      return\n",
        "    \n",
        "    for attr in attrs:\n",
        "      if hasattr(agent, attr) == False:\n",
        "        print(\"ERROR: Agent does not have attribute\", attr)\n",
        "        return\n",
        "      \n",
        "    combinations = 1\n",
        "    for val_list in vals:\n",
        "      combinations *= len(val_list)\n",
        "      \n",
        "    winners = [float(\"-inf\")] * keep_num\n",
        "    \n",
        "    curr_indices = [0] * len(attrs)\n",
        "    curr_indices[len(attrs)-1] = -1 # Account for initial while loop increment.\n",
        "    curr_attr = len(attrs) - 1 # The attribute we're currently iterating over. Start at the 'rightmost' attribute.\n",
        "    \n",
        "    counter = 0\n",
        "    while True: # Loop until all combinations exhausted.\n",
        "      curr_attr = len(attrs) - 1\n",
        "      curr_indices[curr_attr] += 1 # Increment the current attribute by 1.\n",
        "        \n",
        "      while(curr_indices[curr_attr] > len(vals[curr_attr]) - 1): # Overflow case.\n",
        "        curr_indices[curr_attr] = 0\n",
        "        curr_attr -= 1\n",
        "        curr_indices[curr_attr] += 1 # Increment the current attribute by 1.\n",
        "        if curr_attr == -1:\n",
        "          return winners\n",
        "          \n",
        "      # When we reach here, we hopefully have a new combination of parameters.\n",
        "      # Set the agent to have each of these parameters.\n",
        "      counter += 1\n",
        "      print(\"Testing combination\", counter, \"of\", combinations)\n",
        "      \n",
        "      output = \"\"\n",
        "      \n",
        "      for i, attr in enumerate(attrs):\n",
        "        attr_index = curr_indices[i]\n",
        "        val = vals[i][attr_index]\n",
        "        setattr(agent, attr, val)\n",
        "        output = output + str(val) + \" \"\n",
        "        \n",
        "      print(output)\n",
        "      \n",
        "      average = self.runner.runEnv(agent, self.episodes, 100)\n",
        "      print(average)\n",
        "      \n",
        "      # Track the best combinations\n",
        "      if average > winners[0]:\n",
        "        winners[0] = average\n",
        "        winners.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOd8yabUVwE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zO7Sxr0t8DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "runner = EnvRunner('FrozenLake-v0')\n",
        "searcher = GridSearch(runner, 2000)\n",
        "\n",
        "agent = Q_Table_Agent_ep(env.observation_space.n, env.action_space.n, 0.9, 0.8, 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI1UjM7L3b2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        },
        "outputId": "0aea1cfa-e1d2-4646-c572-9a4318dc87fb"
      },
      "source": [
        "results = searcher.search(agent, [\"g\", \"lr\", \"ep\"], [[0.6,0.8,1],[0.75,0.85,0.95],[0.01,0.05,0.1]], 10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing combination 1 of 27\n",
            "0.6 0.75 0.01 \n",
            "0.082\n",
            "Testing combination 2 of 27\n",
            "0.6 0.75 0.05 \n",
            "0.0875\n",
            "Testing combination 3 of 27\n",
            "0.6 0.75 0.1 \n",
            "0.0675\n",
            "Testing combination 4 of 27\n",
            "0.6 0.85 0.01 \n",
            "0.0935\n",
            "Testing combination 5 of 27\n",
            "0.6 0.85 0.05 \n",
            "0.0835\n",
            "Testing combination 6 of 27\n",
            "0.6 0.85 0.1 \n",
            "0.0915\n",
            "Testing combination 7 of 27\n",
            "0.6 0.95 0.01 \n",
            "0.23\n",
            "Testing combination 8 of 27\n",
            "0.6 0.95 0.05 \n",
            "0.0965\n",
            "Testing combination 9 of 27\n",
            "0.6 0.95 0.1 \n",
            "0.0685\n",
            "Testing combination 10 of 27\n",
            "0.8 0.75 0.01 \n",
            "0.1715\n",
            "Testing combination 11 of 27\n",
            "0.8 0.75 0.05 \n",
            "0.1375\n",
            "Testing combination 12 of 27\n",
            "0.8 0.75 0.1 \n",
            "0.1085\n",
            "Testing combination 13 of 27\n",
            "0.8 0.85 0.01 \n",
            "0.114\n",
            "Testing combination 14 of 27\n",
            "0.8 0.85 0.05 \n",
            "0.184\n",
            "Testing combination 15 of 27\n",
            "0.8 0.85 0.1 \n",
            "0.1165\n",
            "Testing combination 16 of 27\n",
            "0.8 0.95 0.01 \n",
            "0.3385\n",
            "Testing combination 17 of 27\n",
            "0.8 0.95 0.05 \n",
            "0.139\n",
            "Testing combination 18 of 27\n",
            "0.8 0.95 0.1 \n",
            "0.0885\n",
            "Testing combination 19 of 27\n",
            "1 0.75 0.01 \n",
            "0.019\n",
            "Testing combination 20 of 27\n",
            "1 0.75 0.05 \n",
            "0.0775\n",
            "Testing combination 21 of 27\n",
            "1 0.75 0.1 \n",
            "0.0705\n",
            "Testing combination 22 of 27\n",
            "1 0.85 0.01 \n",
            "0.0215\n",
            "Testing combination 23 of 27\n",
            "1 0.85 0.05 \n",
            "0.0685\n",
            "Testing combination 24 of 27\n",
            "1 0.85 0.1 \n",
            "0.076\n",
            "Testing combination 25 of 27\n",
            "1 0.95 0.01 \n",
            "0.3625\n",
            "Testing combination 26 of 27\n",
            "1 0.95 0.05 \n",
            "0.059\n",
            "Testing combination 27 of 27\n",
            "1 0.95 0.1 \n",
            "0.085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biOHqB5614Me",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb8ce8a7-a847-4067-a9ec-93600520bc03"
      },
      "source": [
        "results"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1085, 0.114, 0.1165, 0.1375, 0.139, 0.1715, 0.184, 0.23, 0.3385, 0.3625]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUIV0q7v-ykJ",
        "colab_type": "text"
      },
      "source": [
        "# TO DO\n",
        "* Experiment with gamma, learning rate, and epsilon values (grid search could be automated, read articles?)\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 1",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_Reinforcement_Learning_with_Tensorflow/Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8jPAE1Pz03",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Two-armed Bandit [[Link]](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nl592I4P7yM",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement learning needs a different mindset than typical supervised learning; the 'answer' space is much broader. There's no one 'correct' action for an agent to take, but we'll still find ways to learn nonetheless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d71P3H0nQY6u",
        "colab_type": "text"
      },
      "source": [
        "The **two-armed bandit**, or more broadly $ n$-armed bandit, is one of the simplest RL problems. We have $n$ slot machines, each with some payout probability, so we have to find the best machine and then maximize our reward by always choosing it. In the case of two machines, we have quite a simple problem, but aspects found in many other RL problems include:\n",
        "* Different actions yield different rewards.\n",
        "* Rewards are delayed over time, so we won't immediately know the value of our actions.\n",
        "* The reward for an action depends on the current state of the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT0AliWcRha5",
        "colab_type": "text"
      },
      "source": [
        "The goal of learning which actions are best and ensuring we choose such actions is called learning a **policy**. In this section, we'll be using a method called **policy gradients**, where a simple ANN uses gradient descent to learn which actions to pick. An alternative to this would be learning **value functions**, where our agent learns to predict how good a given state or action will be (the value of the state/action)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJZGgpNSO5U",
        "colab_type": "text"
      },
      "source": [
        "**Policy Gradient**\n",
        "\n",
        "In the simplest case, suppose our network produces explicit outputs. We can ask the network for an output weight for each possible arm to pull, and we'll pick the arm with the highest given weight. To update the network, we'll try arms using an e-greedy policy. See Part 0 for my notes on this algorithm, but it's quite simple (pick a random arm with probability $\\epsilon$, else pick highest weight arm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Disb6viHUBae",
        "colab_type": "text"
      },
      "source": [
        "We'll give our agent a reward of either -1 or 1, and then update the network with equation:\n",
        "\n",
        "$\\text{Loss}=-\\log(\\pi)*A$, where $A$ is the **advantage**. This is an essential part of all RL algorithms which corresponds to how much better an action was than some baseline. For now we assume the baseline is 0, so the advantage will just be the reward we recieve. $\\pi$ is our policy, which here means the weight of the chosen action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE-iQpycUClB",
        "colab_type": "text"
      },
      "source": [
        "Consider this loss function. Say we chose a good action with high confidence: reward 1, weight 0.8. Thus $A=1,\\pi=0.8\\implies\\text{Loss}=-\\log(0.8)*1=0.22$.\n",
        "\n",
        "As for high confidence, bad reward: $A=-1,\\pi=0.8\\implies\\text{Loss}=-\\log(0.8)*-1=-0.22$.\n",
        "\n",
        "For low confidence, good reward: $A=1,\\pi=0.1\\implies\\text{Loss}=-\\log(0.1)*1=2.3$.\n",
        "\n",
        "We see that the agent will increase the weight for actions with positive reward, choosing those actions more frequently in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_57jhi3V0U4",
        "colab_type": "text"
      },
      "source": [
        "Now let's write the code for this problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl6Vwf8VzRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "bandits = [0.1,0.4,0.7,0.99]\n",
        "\n",
        "def pullBandit(bandit):\n",
        "  # Returns a good reward with odds of the given bandit.\n",
        "  r = random.random()\n",
        "  if r < bandit:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1FockqnXp31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
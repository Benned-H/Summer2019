{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 1: Introduction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Speech%20and%20Language%20Processing/Chapter_1_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MynlS1BD_h_F",
        "colab_type": "text"
      },
      "source": [
        "# Introduction (p. 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiKBN01Z_jbz",
        "colab_type": "text"
      },
      "source": [
        "As of 2007, this field went by the names of *computer speech and language processing* or *human language technology* or *natural language processing* or *computational linguistics*. Whatever you call it, we want computers to process human language in useful ways. Some of these ways include:\n",
        "* Conversational agents/dialogue systems - Can converse with humans via natural language.\n",
        "* Machine translation - Automatic translation between languages.\n",
        "* Question answering - As simple as definitions or factoids to as complex as inference or summarization.\n",
        "* Spell or grammar-checking too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJIznDU5AyVW",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Knowledge in Speech and Language Processing (p. 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHC0-yjYA65f",
        "colab_type": "text"
      },
      "source": [
        "A few domains of linguistics are necessary to create HAL, for example:\n",
        "* Phonetics - How words are pronounced in terms of sequences of sounds.\n",
        "* Phonology - How these sounds are realized acoustically.\n",
        "* Morphology - The way words break down into component parts.\n",
        "* Syntax - The knowledge needed to order and group words together.\n",
        "* Lexical Semantics - The meaning of words.\n",
        "* Compositional semantics - What constitutes 'Western Europe' or 'end of the 18th century'?\n",
        "* Conversation knowledge - Is a given utterance a request for action, statement, or a question?\n",
        "* Pragmatics - Knowledge about dialogue and the appropriate lexicon for a given context.\n",
        "* Discourse knowledge - Includes coreference resolution and context understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yg8vHQHKY2a",
        "colab_type": "text"
      },
      "source": [
        "Concretely, these areas can focus into:\n",
        "* Phonetics and Phonology - Knowledge about linguistic sounds\n",
        "* Morphology - Knowledge about the meaningful components of words\n",
        "* Syntax - Knowledge of the structural relationships of words\n",
        "* Semantics - Knowledge of meaning\n",
        "* Pragmatics - Knowledge of the relationship between intentions and meaning\n",
        "* Discourse - Knowledge about linguistic units larger than a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM9q2-GXLHXw",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Ambiguity (p. 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghJBkD4fLJTC",
        "colab_type": "text"
      },
      "source": [
        "Surprise surprise, we often have ambiguity and in fact most tasks can be viewed as resolving ambiguity at one of these levels. An input is **ambiguous** if there are multiple alternative linguistic structures that can be built for it. By breaking the various ambiguities into different processes, we can get a grasp on the chaos of possibilities. Some of these processes include:\n",
        "* Lexical disambiguation - Part-of-speech tagging, word sense disambiguation\n",
        "* Syntactic disambiguation - Probabilistic parsing\n",
        "* Speech act interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwlAG4CCM0rk",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Models and Algorithms (p. 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbP53F3dM3ZO",
        "colab_type": "text"
      },
      "source": [
        "Certain models prove quite useful for these tasks, including **state machines**, **rule systems**, **logic**, **probabilistic models**, and **vector-space models**. Algorithms built with these such as **state space search** and **dynamic programming** turn out to be quite useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7nPh0G1Naip",
        "colab_type": "text"
      },
      "source": [
        "Most simply, **state machines** are a type of formal model with states, transitions between states, and an input representation. Variations include **DFAs**, **NFAs**, and **finite-state transducers**. **Rule systems** are a related concept that define the grammars the state machines can parse: **regular grammars**, **regular relations**, **CFGs**, and **feature-augmented grammars** are a few. A third model uses **first order logic** (AKA **predicate calculus**) and related formalisms to model semantics and pragmatics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUjEUCYiOWS9",
        "colab_type": "text"
      },
      "source": [
        "In all of these models, a probabilistic nature becomes crucial for capturing linguistic knowledge. One example, the **Markov model**, can be created by adding probabilities to a state machine. The crucial reframing this gives us is to change the problem definition into: \"Out of these $N$ ambiguous choices, choose the most probable one.\" Processing languages through these models involved a search through a space of states representing hypotheses about an input. Heuristic variants such as **best-first** or **A*** rely on dynamic programming to remain computable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REt26fArPqMW",
        "colab_type": "text"
      },
      "source": [
        "Machine learning models also help out, including:\n",
        "* Classifiers - Decision trees, support vector machines, Gaussian Mixture Models, or logistic regression\n",
        "* Sequence models - Hidden Markov model, Maximum Entropy Markov Models, or Conditional Random Fields\n",
        "\n",
        "Methodological tools used in machine learning also help, such as **cross-validation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3McA_6jQQwh",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Language, Thought, and Understanding (p. 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR5_nvkiQgIg",
        "colab_type": "text"
      },
      "source": [
        "*Turing Test* - Turing's suggestion of an empirical test for determining if a computer can think. A human interrogator asks a series of questions to determine which of a human contestant and a machine is the machine. The machine works to respond as a human would.\n",
        "\n",
        "The crucial result/assumption was that using language as humans do would indicate machine intelligence. One early NLP system became relevant to this test as early as 1966: the ELIZA program, which used pattern-matching to process input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTnb4RI04mzA",
        "colab_type": "text"
      },
      "source": [
        "The Loebner Prize competetion has gone on since 1991, putting various computer programs through the Turing test. By the turn of the century, talking about computers as 'thinking' might be left unchallenged in public discourse. Thus we're nearing the present:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ympEleTD5ciD",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 The State of the Art (p. 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYc49GWJ5eAC",
        "colab_type": "text"
      },
      "source": [
        "As of 2007, the state of the art already seemed somewhat impressive. I'll take notes on the SOTA once I'm reading a more current source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hywjfj7J6Cq2",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 Some Brief History (p. 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG7N-sE06GGC",
        "colab_type": "text"
      },
      "source": [
        "Speech and language processing has somehow been treated quite differently between CS, ECE, linguistics, and psych in the past. These intersecting fields have their own names:\n",
        "* **Computational linguistics** from the linguistics viewpoint\n",
        "* **Natural language processing** from the computer science viewpoint\n",
        "* **Speech recognition** from the electrical engineering viewpoint\n",
        "* **Computational psycholinguistics** from the psychology viewpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxf1aDD610w",
        "colab_type": "text"
      },
      "source": [
        "### Foundational Insights: 1940s - 50s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOH-rxxY67AX",
        "colab_type": "text"
      },
      "source": [
        "Just after WW2, the **automaton** and **probabilistic models** were developed. Turing's algorithmic computation (1936) led the the **McCulloch-Pitts neuron** in 1943. Kleene added on with finite automata and RegEx (1951, 56). Markov processes led into Chomsky working on finite-state grammar and CFG (1956).\n",
        "\n",
        "A second major insight came from probabilistic algorithms. Shannon developed the metaphor of the **noisy channel** and **decoding** for transmitting language through media. By 1952, Bell Labs had a system that could recognize the 10 digits from a single speaker with 97-99% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1PnbJrD8fUE",
        "colab_type": "text"
      },
      "source": [
        "### The Two Camps: 1957 - 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orJnELG48kpV",
        "colab_type": "text"
      },
      "source": [
        "By the end of the 50s, the field had split into two paradigms: symbolic and stochastic. The symbolic came from Chomsky and other's work on formal language syntax and parsing. One of the earliest parsing systems was Zelig Harris' TDAP in 1959. Also included was work on reasoning and logic.\n",
        "\n",
        "The stochastic paradigm was popular among statistics and ECE departments. Bayesian methods were being successfully applied by 1959. Also there were also online corpora by 1964."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prt3Bw7P9vMG",
        "colab_type": "text"
      },
      "source": [
        "### Four Paradigms: 1970 - 83"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tVCaxE79zJu",
        "colab_type": "text"
      },
      "source": [
        "1. Stochastic - Included the Hidden Markov Model and played huge role in speech recognition algorithms.\n",
        "2. Logic-based - Begun by Colmerauer, included Q-systems, metamorphosis grammars, Definite Clause Grammars, and LFG.\n",
        "3. Natural language understanding - Began with Winograd's SHRDLU, used human conceptual knowledge.\n",
        "4. Discourse modeling focused on the substructure in discourse, discourse focus, reference resolution, and the belief-desire-intention framework (work by [Allen](https://nlp.stanford.edu/acvogel/allenperrault.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8la_JdD6_dB3",
        "colab_type": "text"
      },
      "source": [
        "### Empiricism and Finite State Models Redux: 1983 - 93"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inZsUp8h_jU1",
        "colab_type": "text"
      },
      "source": [
        "This decade saw the return of finite state models, now used in finite-state phonology, syntax, and morphology. A second trend saw the rise of probabilistic models spread into more domains of the field and better metrics for testing models were developed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKgQZ9RTACLp",
        "colab_type": "text"
      },
      "source": [
        "### The Field Comes Together: 1994 - 99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8qD3C4oAGL5",
        "colab_type": "text"
      },
      "source": [
        "By now probabilistic and data-driven models were quite standard. Speedup in computers allowed commercialization of some subareas of speecha and language processing. The web demonstrated a need for language-based information retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09hhWLWgAsHO",
        "colab_type": "text"
      },
      "source": [
        "### The Rise of Machine Learning: 2000 - 07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqwvwoKvAw1J",
        "colab_type": "text"
      },
      "source": [
        "Three trends caused the empiricist trends to accelerate:\n",
        "1. Large amounts of data became available through the Linguistic Data Consortium and other groups. These labeled examples encouraged supervised learning attempts at parsing and semantic analysis.\n",
        "2. The statistical ML community leaned in with support vector machines (1995), multinomial logistic regression (1996), and graphical Bayesian models (1988).\n",
        "3. Larger unsupervised approaches became more feasible as high-performance computing became more accessible. (Even by 2007, by the way. Wow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhpX5OyB2od",
        "colab_type": "text"
      },
      "source": [
        "The authors note that a number of discoveries developed simultaneously between multiple independent researchers. These are widespread in history and the practices of submission dates for journals, research records, and the circulation of preliminary/techinical reports may help scientists avoid getting \"scooped.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI7UfY93CuNj",
        "colab_type": "text"
      },
      "source": [
        "Finally, the understanding of human language processing can help us better solve this human-related task. If we have the same goals that humans regularly perform, we can learn from nature's implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rmsIDgzDH-a",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 Summary (p. 14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLaWyiYPDJJi",
        "colab_type": "text"
      },
      "source": [
        "See the text for the summary; I've written mine above.\n",
        "\n",
        "*--Finished 5/22/2019--*"
      ]
    }
  ]
}
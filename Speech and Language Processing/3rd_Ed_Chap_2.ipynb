{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3rd Ed. Chap. 2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Speech%20and%20Language%20Processing/3rd_Ed_Chap_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3J32Rtuozw_",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 2: RegEx, Text Normalization, Edit Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrW3D54vqxCU",
        "colab_type": "text"
      },
      "source": [
        "Given that my previous notes often become long-winded, I'm working to keep this down to definitions, explanations, and algorithms. Filler may be cut out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uleL-wJiOLqr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1: Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufymeQGio5qR",
        "colab_type": "text"
      },
      "source": [
        "**Regular expressions** (**RE**) are used to specify text search strings. **Text normalization** is the process of turning text into a more convenient form, which can include **tokenizing** words, or in the case of tweets or texts perhaps **emoticons** like ```:-)``` or **hashtags**. Languages like Chinese may not even have spaces between words, making tokenization more challenging. **Lemmatization** is the process of determining if two words share a root, e.g. *sing*, *sang*, and *sung*. **Stemming** is a simpler task of stripping suffixes off of words. Finally, text normalization can include **sentence segmentation**: breaking the text into sentences using punctuation cues. To compare words/strings, we'll introduce **edit distance** to measure how similar two strings are based on the number of edits between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOluE9nbrAPn",
        "colab_type": "text"
      },
      "source": [
        "We can specify **patterns** using RE to search a **corpus** of texts for all texts that match the pattern. Searches can be designed to return only first matches, full lines of documents, or entire documents. **Extended RE** gives easier syntax for patterns that simpler RE might need longer notation to specify. Here's the syntax:\n",
        "* ```/str/``` specifies instances of the entered string, case sensitive.\n",
        "* **Disjunction of characters**: ```/[abc]/``` specifies patterns containing either *a*, *b*, or *c*.\n",
        "* **Range**: ```/[0-9]/``` is equivalent to ```/0123456789/```, ```/[3-7]/``` specifies *3*, *4*, *5*, *6*, or *7*, or ```/[A-Z]/``` specifies any uppercase letter.\n",
        "* **Negation**: A caret after the bracket negates a disjunction, so ```/[^aA]/``` specifies any character expect *a* or *A*.\n",
        "* Use ```/?/``` for \"zero or one instance of the previous character, e.g. ```/boys?/``` finds *boy* or *boys*\n",
        "* **Kleene \\***: Means \"zero or more instances of previous character or RE\", e.g. an integer is ```/[0-9][0-9]*/```, as we need at least one digit.\n",
        "* **Kleene +**: Means \"one or more of previous character or RE\", e.g. an integer is ```/[0-9]+/```.\n",
        "* **Wildcard**: ```/./``` matches any single character, except a carriage return. Use a backslash to specify a regular period. Consider ```/.*/```.\n",
        "* **Anchors** are special characters that require certain places in a string.\n",
        ">a. The caret ```/^/``` matches the start of a line, so ```/^The/``` will only match *The* when it begins its line.   \n",
        ">b. The dollar sign ```/$/``` matches the end of a line.   \n",
        ">c. ```\\b``` matches a word boundary. \"Word\" here means a sequence of digits, underscores, or letters. Hence ```/\\bthe\\b/``` will match *the* but not *other*.   \n",
        ">d. ```\\B``` matches a non-boundary.\n",
        "* **Disjunction**: The pipe symbol means \"or\", e.g. ```/cat|dog/``` specifying *cat* or *dog*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVfzeRyvx26R",
        "colab_type": "text"
      },
      "source": [
        "Clearly sequences take precedence over disjunction, but we can use parentheses to edit this behavior. Enclosing a RE in parentheses makes it act like a single character for surrounding operators. Thus ```/bab(y|ies)/``` specifies either the singular or plural. This enables the use of the Kleene star on expressions. We can formalize this precedence with a **operator precedence hierarchy**, showing which operators apply first:\n",
        "1. Parenthesis: ```()```\n",
        "2. Counters: ```* + ? {}```\n",
        "3. Sequences/anchors: ```the ^my end$```\n",
        "4. Disjunction: ```|```\n",
        "\n",
        "Note that RE match the largest expression they can: they're **greedy**. We can specify **non-greedy** matching using the ```*?``` operator, a Kleene star that matches as little text as possible. ```+?``` is the same for a Kleene plus. As we use these operators to find desired patterns, we want to fix two kinds of errors: **false positives** and **false negatives**. Increasing **precision** means minimizing false positives, and increasing **recall** means minimizing false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r9e3Hb1Ltqk",
        "colab_type": "text"
      },
      "source": [
        "There are a few more RE operators:\n",
        "* **Curly brackets** can be used to specify given numbers of REs:   \n",
        ">a. ```/{2}/``` matches exactly 2 occurrences of previous RE.   \n",
        ">b. ```/{n,m}/``` matches from *n* to *m* occurrences of previous RE.   \n",
        ">c. ```/{n,}/``` matches at least *n* occurrences of previous RE.   \n",
        ">d. ```/{,m}/``` matches up to *m* occurrences of previous RE.\n",
        "* ```\\d``` matches any digit; it's equivalent to ```[0-9]```.\n",
        "* ```\\D``` matches any non-digit; equivalent to ```[^0-9]```.\n",
        "* ```\\w``` matches any alphanumeric or underscore.\n",
        "* ```\\W``` matches a non-alphanumeric or underscore; ```[^\\w]```.\n",
        "* ```\\s``` matches any whitespace (space, tab).\n",
        "* ```\\S``` matches non-whitespace; ```[^\\s]```.\n",
        "* **Backslash**: Newline is ```\\n```, tab is ```\\t```. An asterisk, period, bracket, or question mark can all be referred to using a backslash. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOnbA6HYOQR5",
        "colab_type": "text"
      },
      "source": [
        "**Substitution** is an important use of RE. The substitution operator ```s/regexp1/pattern/``` replaces strings characterized by the RE to the other string. We can refer to string we find using parentheses on the first operator, and then use the **number** operator ```\\1``` to refer back to it. For example:   \n",
        "```s/([0-9]+)/<\\1>/```. This puts angle brackets around all integers. We can specify that a certain string must occur twice in a text by using ```\\1``` in the place of the second instance. The use of parentheses to stores these patterns is called a **capture group**. Each time parentheses surround a pattern, the match is stored in a numbered **register**. Thus we can use ```\\2``` and then higher numbers for additional capture groups. Using parentheses for only grouping is called a **non-capturing group**, which is specified by putting ```?:``` after the open paren, in the form ```(?:pattern)```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5zfSZunPFL2",
        "colab_type": "text"
      },
      "source": [
        "**Lookahead** assertions can be used to check for some pattern without advancing the cursor. The operator ```(?=pattern)``` is true if the pattern occurs, but it's **zero-width**, i.e. it doesn't advance the match pointer. Similarly, ```(?!pattern)``` is true only if the pattern *does not* occur, but it doesn't advance the cursor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjbWWJniSpp9",
        "colab_type": "text"
      },
      "source": [
        "## 2.2: Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbfP0ckUSrTZ",
        "colab_type": "text"
      },
      "source": [
        "A **corpus** is a computer-readable collection of text or speech. As we parse text from some corpus, whether we treat punctuation marks as their own words will depend on our task. In spoken language, an **utterance** is the spoken correlate of a sentence. **Disfluencies** are abnormalities that pop up in spoken speech, like **fragments** (broken off words) or **fillers** (words like *uh*, *um*). For text, should we differentiate between capitalized words and their lowercase versions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB6JuEP2UaJX",
        "colab_type": "text"
      },
      "source": [
        "A **lemma** is a set of lexical forms with the same stem, same part-of-speech, and same word sense. The **wordform** is the full inflected or derived form of the word. In languages such as Arabic, we'll often need to deal with lemmatization. **Types** are the number of distinct words in a corpus; vocabulary size $|V|$ equals the number of types. **Tokens** are the number $N$ of total running words in the corpus. Some popular corpora include:   \n",
        ">Corpus | Tokens | Types\n",
        ">:--- | ---: | ---:\n",
        ">Shakespeare | 884k | 31k\n",
        ">Brown corpus | 1mil | 38k\n",
        ">Switchboard telephone conversations | 2.4mil | 20k\n",
        ">COCA | 440mil | 2mil\n",
        ">Google N-grams | 1tril | 13mil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtWp0LhoXQOC",
        "colab_type": "text"
      },
      "source": [
        "The relationship between the number of types and number of tokens is called **Herdan's Law**, or **Heaps' Law** (In linguistics and information retrieval respectively). It's written as:   \n",
        "$|V|=kN^\\beta$, where $k$ and $\\beta$ are positive constants and $0<\\beta<1$. The value of $\\beta$ depends on the corpus and genre, but generally the vocabulary size for a text goes up significantly faster than the square root of its number of tokens. Dictionaries can also give an approximate upper bound on the number of lemmas in a language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBsXQxt5Y3xg",
        "colab_type": "text"
      },
      "source": [
        "## 2.3: Corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIId0E-mY7UA",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the most important dimension of variation is the language; NLP algorithms are most useful when they generalize to other languages. This is also necessary for different dialects, such as African American Vernacular English (**AAVE**), a dialect spoken by millions in the US. Also common is **code switching**, where speakers or writers use multiple languages in a single communicative act. Corpora can also vary by genre, demographic, and historical period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI3HiIwYaCQ7",
        "colab_type": "text"
      },
      "source": [
        "## 2.4: Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBPKIqbpaE8y",
        "colab_type": "text"
      },
      "source": [
        "At least three tasks are typical for a normalization process:\n",
        "1. Segmenting/tokenizing words from running text.\n",
        "2. Normalizing word formats.\n",
        "3. Segmenting sentences from running text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2hGguAaXXd",
        "colab_type": "text"
      },
      "source": [
        "**Example in Unix commandline** - We'll use ```tr``` to systematically change characters in the input, ``sort`` to sort input lines in alphabetical order, and ``uniq``, which collapses and counts adjecent identical lines.\n",
        "\n",
        "1. Use ```tr``` to translate characters. The format ```tr -tags str1 str2``` copies the standard input to the standard output with the characters in ```str1``` substituted to ```str2```. The ```-c``` tag *complements* the values in ```str1``` and the ```-s``` tag *squeezes* all sequences of ```str2``` in the output to a single instance.   \n",
        ">Thus ```tr -sc \"A-Za-z\" \"\\n\" < shakespeare.txt``` splits the corpus into individual alphabetic words on each line. We use the ```<``` to pass the text as input.\n",
        "2. The ```sort``` utility sorts text by line, and we use a pipe ```|``` to take the input of ```tr``` and pass it to the next utility.\n",
        "3. We then use ```uniq``` to read adjecent lines of the input and writes copies to the output, except duplicate lines aren't written. The ```-c``` tag precedes each line with a count of its occurances.   \n",
        ">This gives us ```tr -sc \"A-Za-z\" \"\\n\" < shakespeare.txt | sort | uniq -c``` so far.\n",
        "4. We could also use ```tr A-Z a-z``` to convert all text to lowercase before sorting it.\n",
        "5. Finally, we can use the ```sort``` tags ```-n``` to consider lines numerically and ```-r``` to sort in reverse order, giving us the largest values first. I also put this into a file to save our results. Our final command is:   \n",
        ">```tr -sc \"A-Za-z\" \"\\n\" < shakespeare.txt | tr A-Z a-z | sort | uniq -c | sort -n -r > word_counts.txt```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tTyn6mQ32OG",
        "colab_type": "text"
      },
      "source": [
        "We'll need more advanced tools for **tokenization**, the task of segmenting running text into words, and **normalization**, the task of putting words/tokens into a standard format. We'll typically want to keep punctuation as tokens, but keep internal punctuation in words intact (e.g. *Ph.D.*, *$45.55*, *03/16/1999*, or *#nlproc*). We can also expand **clitic** contractions marked by apostrophes. Clitics are parts of words that can't stand on their own. We might also need to consider multiword expressions, which links this task to **named entity detection**, the task of detecting names, dates, and organizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNVSya-35XaS",
        "colab_type": "text"
      },
      "source": [
        "A common tokenization standard is the **Penn Treebank tokenization** standard, used for parsed corpora released by the Linguistic Data Consortium (LDC). This separates clitics, keeps hyphenated words together, and separates out all punctuation. Tokens can be **normalized**, where a single form is chosen for words with multiple forms, e.g. *US* and *USA*. **Case folding** normalizes to lowercase, which might be desirable in speech recognition or IR, but maybe not sentiment analysis, IE, or MT. In practice, tokenization needs to be fast and deterministic, as it needs to happen before all other language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi3UzPhY6-Gg",
        "colab_type": "text"
      },
      "source": [
        "In languages such as written Chinese, Japanese, and Thai, there are no spaces to mark word boundaries. Chinese words are composed of **hanzi** characters which are generally pronounced as a single morpheme. A simple algorithm called **maximum matching** (**MaxMatch**) performs quite well for segmenting Chinese, although it requires a wordlist of the language. It starts pointing at the beginning of the string and chooses the longest word in the dictionary currently matching the input. The pointer is advanced to the end of that string and the process repeats. If no word matches, advance the pointer a character (this will be a one-character word) and re-apply again. My version of the pseudocode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J21OC6fj8qRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxMatch(text,dictionary):\n",
        "  # Returns a list of words in the sentence.\n",
        "  if len(text) == 0:\n",
        "    return []\n",
        "  length = len(text)\n",
        "  while length != 0: # All possible lengths will be checked.\n",
        "    first_word = text[0:length]\n",
        "    remainder = text[length:len(text)] # Rest of sentence.\n",
        "    if first_word in dictionary:\n",
        "      # Concatenate the found word with rest of sentence's result.\n",
        "      return [first_word] + maxMatch(remainder,dictionary)\n",
        "    length = length - 1\n",
        "\n",
        "  # No word was found, so create a one-character word.\n",
        "  first_word = text[0]\n",
        "  remainder = text[1:]\n",
        "  return [first_word] + maxMatch(remainder,dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plsdC_4H-iUe",
        "colab_type": "text"
      },
      "source": [
        "This doesn't work as well on English, because Chinese has much shorter words. We can quantify how well the segmenter works using a metric called **word error rate**, where we compare our result with a perfect hand-segmented (i.e. **gold**) sentence. The WER is the normalized minimum edit distance between the two: the number of word insertions, deletions, and substitutions divided by the length of the gold sentence in words. We'll see how to calculate this soon."
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_RL_with_TF/Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8jPAE1Pz03",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Two-armed Bandit [[Link]](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nl592I4P7yM",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement learning needs a different mindset than typical supervised learning; the 'answer' space is much broader. There's no one 'correct' action for an agent to take, but we'll still find ways to learn nonetheless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d71P3H0nQY6u",
        "colab_type": "text"
      },
      "source": [
        "The **two-armed bandit**, or more broadly $ n$-armed bandit, is one of the simplest RL problems. We have $n$ slot machines, each with some payout probability, so we have to find the best machine and then maximize our reward by always choosing it. In the case of two machines, we have quite a simple problem, but aspects found in many other RL problems include:\n",
        "* Different actions yield different rewards.\n",
        "* Rewards are delayed over time, so we won't immediately know the value of our actions.\n",
        "* The reward for an action depends on the current state of the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT0AliWcRha5",
        "colab_type": "text"
      },
      "source": [
        "The goal of learning which actions are best and ensuring we choose such actions is called learning a **policy**. In this section, we'll be using a method called **policy gradients**, where a simple ANN uses gradient descent to learn which actions to pick. An alternative to this would be learning **value functions**, where our agent learns to predict how good a given state or action will be (the value of the state/action)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJZGgpNSO5U",
        "colab_type": "text"
      },
      "source": [
        "**Policy Gradient**\n",
        "\n",
        "In the simplest case, suppose our network produces explicit outputs. We can ask the network for an output weight for each possible arm to pull, and we'll pick the arm with the highest given weight. To update the network, we'll try arms using an $\\epsilon$-greedy policy. See Part 0 for my notes on this algorithm, but it's quite simple (pick a random arm with probability $\\epsilon$, else pick highest weight arm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Disb6viHUBae",
        "colab_type": "text"
      },
      "source": [
        "We'll give our agent a reward of either -1 or 1, and then update the network with equation:\n",
        "\n",
        "$\\text{Loss}=-\\log(\\pi)*A$, where $A$ is the **advantage**. This is an essential part of all RL algorithms which corresponds to how much better an action was than some baseline. For now we assume the baseline is 0, so the advantage will just be the reward we recieve. $\\pi$ is our policy, which here means the weight of the chosen action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE-iQpycUClB",
        "colab_type": "text"
      },
      "source": [
        "Consider this loss function. Say we chose a good action with high confidence: reward 1, weight 0.8. Thus $A=1,\\pi=0.8\\implies\\text{Loss}=-\\log(0.8)*1=0.22$.\n",
        "\n",
        "As for high confidence, bad reward: $A=-1,\\pi=0.8\\implies\\text{Loss}=-\\log(0.8)*-1=-0.22$.\n",
        "\n",
        "For low confidence, good reward: $A=1,\\pi=0.1\\implies\\text{Loss}=-\\log(0.1)*1=2.3$.\n",
        "\n",
        "We see that the agent will increase the weight for actions with positive reward, choosing those actions more frequently in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHzHmgsw5TNT",
        "colab_type": "text"
      },
      "source": [
        "# Learning Some TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rgYtr-s5WaV",
        "colab_type": "text"
      },
      "source": [
        "It's at this point that the reinforcement learning tutorial throws some code below the article and calls it a day. I have no idea how to use TensorFlow placeholders or optimizers; my TensorFlow experience is minimal. I'll go through a few sources on TensorFlow basics here before tackling the problem at hand again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm2Q-Uus5XiD",
        "colab_type": "text"
      },
      "source": [
        "# Back to the RL Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_57jhi3V0U4",
        "colab_type": "text"
      },
      "source": [
        "Now let's write the code for this problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl6Vwf8VzRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "bandits = [0.1,0.4,0.7,0.99]\n",
        "num_bandits = len(bandits)\n",
        "\n",
        "def pullBandit(bandit):\n",
        "  # Returns a good reward with odds of the given bandit.\n",
        "  r = random.random()\n",
        "  if r < bandit:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1FockqnXp31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "ea30e73e-c08d-42e0-c048-3d0693199b48"
      },
      "source": [
        "class PolicyGradient_ep:\n",
        "  \"\"\"A policy gradient agent that chooses epsilon-greedy actions.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_actions, ep):\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-6df895e34851>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqLVwRK636Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#These two lines established the feed-forward part of the network. This does the actual choosing.\n",
        "weights = tf.Variable(tf.ones([num_bandits]))\n",
        "chosen_action = tf.argmax(weights,0)\n",
        "\n",
        "#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
        "#to compute the loss, and use it to update the network.\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "responsible_weight = tf.slice(weights,action_holder,[1])\n",
        "loss = -(tf.log(responsible_weight)*reward_holder)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAm1PtM84AYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0c6a8642-eae0-4502-e36d-c7bfce1497dc"
      },
      "source": [
        "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
        "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
        "e = 0.01 #Set the chance of taking a random action.\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while i < total_episodes:\n",
        "        \n",
        "        #Choose either a random action or one from our network.\n",
        "        if np.random.rand(1) < e:\n",
        "            action = np.random.randint(num_bandits)\n",
        "        else:\n",
        "            action = sess.run(chosen_action)\n",
        "        \n",
        "        reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
        "        \n",
        "        #Update the network.\n",
        "        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
        "        \n",
        "        #Update our running tally of scores.\n",
        "        total_reward[action] += reward\n",
        "        if i % 50 == 0:\n",
        "            print(\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
        "        i+=1\n",
        "print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "if np.argmax(ww) == np.argmax(np.array(bandits)):\n",
        "    print(\"...and it was right!\")\n",
        "else:\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 bandits: [-1.  0.  0.  0.]\n",
            "Running reward for the 4 bandits: [-1. -2. -1. 43.]\n",
            "Running reward for the 4 bandits: [-1. -2. -1. 89.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 139.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 185.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 233.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 279.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 329.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.   0. 376.]\n",
            "Running reward for the 4 bandits: [ -1.  -1.   0. 421.]\n",
            "Running reward for the 4 bandits: [ -1.  -1.   0. 471.]\n",
            "Running reward for the 4 bandits: [ -1.   0.   0. 518.]\n",
            "Running reward for the 4 bandits: [ -1.   0.   0. 564.]\n",
            "Running reward for the 4 bandits: [ -2.   0.   0. 611.]\n",
            "Running reward for the 4 bandits: [ -3.   0.   0. 658.]\n",
            "Running reward for the 4 bandits: [ -3.   0.   0. 706.]\n",
            "Running reward for the 4 bandits: [ -3.   0.   0. 756.]\n",
            "Running reward for the 4 bandits: [ -4.   0.   0. 803.]\n",
            "Running reward for the 4 bandits: [ -4.   0.   0. 853.]\n",
            "Running reward for the 4 bandits: [ -4.  -1.  -1. 901.]\n",
            "The agent thinks bandit 4 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwum8PTA56NS",
        "colab_type": "text"
      },
      "source": [
        "The above code is from the RL tutorial; I need to learn pieces of TensorFlow before reading through this. Links to do so:\n",
        "https://appdividend.com/2019/02/06/tensorflow-variables-and-placeholders-tutorial-with-example/\n",
        "https://www.edureka.co/blog/tensorflow-tutorial/\n",
        "\n",
        "That's next time. Last revised 6/14/2019"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_RL_with_TF/Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8jPAE1Pz03",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Two-armed Bandit [[Link]](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nl592I4P7yM",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement learning needs a different mindset than typical supervised learning; the 'answer' space is much broader. There's no one 'correct' action for an agent to take, but we'll still find ways to learn nonetheless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d71P3H0nQY6u",
        "colab_type": "text"
      },
      "source": [
        "The **two-armed bandit**, or more broadly $ n$-armed bandit, is one of the simplest RL problems. We have $n$ slot machines, each with some payout probability, so we have to find the best machine and then maximize our reward by always choosing it. In the case of two machines, we have quite a simple problem, but aspects found in many other RL problems include:\n",
        "* Different actions yield different rewards.\n",
        "* Rewards are delayed over time, so we won't immediately know the value of our actions.\n",
        "* The reward for an action depends on the current state of the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT0AliWcRha5",
        "colab_type": "text"
      },
      "source": [
        "The goal of learning which actions are best and ensuring we choose such actions is called learning a **policy**. In this section, we'll be using a method called **policy gradients**, where a simple ANN uses gradient descent to learn which actions to pick. An alternative to this would be learning **value functions**, where our agent learns to predict how good a given state or action will be (the value of the state/action)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJZGgpNSO5U",
        "colab_type": "text"
      },
      "source": [
        "**Policy Gradient**\n",
        "\n",
        "In the simplest case, suppose our network produces explicit outputs. We can ask the network for an output weight for each possible arm to pull, and we'll pick the arm with the highest given weight. To update the network, we'll try arms using an $\\epsilon$-greedy policy. See Part 0 for my notes on this algorithm, but it's quite simple (pick a random arm with probability $\\epsilon$, else pick highest weight arm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Disb6viHUBae",
        "colab_type": "text"
      },
      "source": [
        "We'll give our agent a reward of either -1 or 1, and then update the network with equation:\n",
        "\n",
        "$\\text{Loss}=-\\log(\\pi)*A$, where $A$ is the **advantage**. This is an essential part of all RL algorithms which corresponds to how much better an action was than some baseline. For now we assume the baseline is 0, so the advantage will just be the reward we recieve. $\\pi$ is our policy, which here means the weight of the chosen action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE-iQpycUClB",
        "colab_type": "text"
      },
      "source": [
        "Consider this loss function. Say we chose a good action with high confidence: reward 1, weight 0.8. Thus $A=1,\\pi=0.8\\implies\\text{Loss}=-\\log(0.8)*1=0.22$.\n",
        "\n",
        "As for high confidence, bad reward: $A=-1,\\pi=0.8\\implies\\text{Loss}=-\\log(0.8)*-1=-0.22$.\n",
        "\n",
        "For low confidence, good reward: $A=1,\\pi=0.1\\implies\\text{Loss}=-\\log(0.1)*1=2.3$.\n",
        "\n",
        "We see that the agent will increase the weight for actions with positive reward, choosing those actions more frequently in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHzHmgsw5TNT",
        "colab_type": "text"
      },
      "source": [
        "# Learning Some TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rgYtr-s5WaV",
        "colab_type": "text"
      },
      "source": [
        "It's at this point that the reinforcement learning tutorial throws some code below the article and calls it a day. I have no idea how to use TensorFlow placeholders or optimizers; my TensorFlow experience is minimal. I'll go through a few sources on TensorFlow basics here before tackling the problem at hand again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z-oSqEB1cmx",
        "colab_type": "text"
      },
      "source": [
        "## 1. TensorFlow Tutorial - Deep Learning Using TF [[Link]](https://www.edureka.co/blog/tensorflow-tutorial/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b20ukbLN2Qvd",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow (TF) is an open source deep learning library based on the concept of data flow graphs. This post will discuss what TF is, its code basics, and a use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNv8XI6W2i_N",
        "colab_type": "text"
      },
      "source": [
        "**What are Tensors?**   \n",
        "Tensors are the de facto representation of data in deep learning. Tensors are multidimensional arrays, so in fact a vector is a 1D tensor, matrix 2D tensor, and a 3D array of data is just a 3D tensor. It's that simple. Thus TensorFlow is just a series of operations on a flow of tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Ar410L3G69",
        "colab_type": "text"
      },
      "source": [
        "**What is TensorFlow?**   \n",
        "TensorFlow is a Python library that provides various functionalities for implementing deep learning models. The process of writing a TensorFlow program is composed of two steps:\n",
        "1. Build a computational graph.\n",
        "2. Run the computational graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enMkqp_G4FXP",
        "colab_type": "text"
      },
      "source": [
        "**1. Building a Computational Graph**   \n",
        "A computational graph is a series of TensorFlow operations arranged as nodes in a graph. Each node takes 0 or more tensors as input and produces some tensor output. Here's a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh55zgLb4bdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "a = tf.constant(8.0)\n",
        "b = tf.constant(2.0)\n",
        "\n",
        "c = a*b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjDrLzLr4tYq",
        "colab_type": "text"
      },
      "source": [
        "**Constant nodes** store constant values that take no input, but produce their stored values as output. Above, ```a``` and ```b``` each store the constants ```8.0``` and ```2.0```, respectively. The ```c``` node multiplies ```a``` with ```b```, so executing ```c``` will produce that result. So far we haven't actually computed anything, only described the computation. The computational graph of TensorFlow is just an alternative way to conceptualize mathematical calculations, but this format allows parallelization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8b5_v-650nf",
        "colab_type": "text"
      },
      "source": [
        "**2. Running a Computational Graph**   \n",
        "In order to get the output of node ```c```, we need to run the graph within a **session**, which places the graph onto a CPU or GPU and executes the calculations. A session controls the entire TensorFlow runtime, storing the order in which the operations will be performed and passing results to the next nodes in the pipeline. Here's the code to do this for our example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClHMnAfM6nPA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73bd1863-ff62-42d5-9f47-90e1e922fae4"
      },
      "source": [
        "sess = tf.Session() # Create the session object\n",
        "output_c = sess.run(c) # Run the graph for c, store the result.\n",
        "print(output_c) # Print the output we got from c.\n",
        "sess.close() # Close session to free resources."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFEZESq3566K",
        "colab_type": "text"
      },
      "source": [
        "**Constants, Placeholder and Variables**   \n",
        "In TensorFlow, constants, placeholders and variables each represent different parameters of a deep learning model.   \n",
        "* **Placeholders** allow your graph to take external inputs as parameters. They contain no data and must be fed inputs at runtime. If given no input, they'll generate an error.\n",
        "* **Variables** allow the graph to give a different output w.r.t. some same input. These are the trainable parameters in the graph, which can change over time. They're defined with an initial value and data type (if no type provided, will be inferred from the input)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1DG5vKo7taX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9d87a71-4549-4c76-b20e-46b66c4536ab"
      },
      "source": [
        "# Placeholder Example\n",
        "a = tf.placeholder(tf.float32) # Create 2 placeholders.\n",
        "b = tf.placeholder(tf.float32)\n",
        "\n",
        "mul = a*b\n",
        "sess = tf.Session()\n",
        "output = sess.run(mul, {a: 5, b: 6})\n",
        "print(\"Multiplying a and b:\", output)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multiplying a and b: 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlGx6cp38ZMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8723d51-0d40-427e-f770-52beb5c28c32"
      },
      "source": [
        "# We didn't specify a shape for a and b, so we can do:\n",
        "output = sess.run(mul, {a: [5,6,7], b: [6,7,8]})\n",
        "print(\"Multiplying a and b:\", output)\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multiplying a and b: [30. 42. 56.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba03CzGW9cKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variable Example\n",
        "var = tf.Variable([9.0], dtype = tf.float32)\n",
        "\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyTZrKy4-U_5",
        "colab_type": "text"
      },
      "source": [
        "Variables are *not initialized when you call* ```tf.Variable``` but rather you must use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQzXRJ2r-fDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) # Initialize before using the graph.\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvQvMoVj8nXl",
        "colab_type": "text"
      },
      "source": [
        "Variables are in-memory buffers that contain tensors, and they'll survive across multiple executions of a graph. Normal tensors are instantiated and then immediately deleted after the execution of their graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS_WpRlN_WR3",
        "colab_type": "text"
      },
      "source": [
        "**Example: Linear Regression using TF**   \n",
        "Recall the classic, iconic formula $y=mx+b$. Let's code an algorithm to find a line of best fit for some given points. We'll need:\n",
        "* Dependent/output variable ($y$)\n",
        "* Slope variable ($m$)\n",
        "* y-intercept or bias ($b$)\n",
        "* Independent/input variable ($x$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9rWc6cpBCdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26305a62-d7dd-4c38-9c09-812eb2e5d7b7"
      },
      "source": [
        "# Model parameters, initialize values to +-0.4\n",
        "m = tf.Variable([0.4], dtype = tf.float32) # Slope var\n",
        "b = tf.Variable([-0.4], dtype = tf.float32) # Bias var\n",
        "\n",
        "# Model input\n",
        "x = tf.placeholder(tf.float32) # Independent input\n",
        "\n",
        "# Equation for our predicted output\n",
        "y_hat = m*x + b\n",
        "\n",
        "# Init everything\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Print our current results.\n",
        "print(sess.run(y_hat, {x: [-3,-2,1,2.5]}))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.6 -1.2  0.   0.6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PT9Sv0qDJN-",
        "colab_type": "text"
      },
      "source": [
        "To complete this model, we need to add two things:\n",
        "1. Provide a mechanism for the model to train itself given a set of inputs and outputs.\n",
        "2. Validate our trained model by comparing its output with desired outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-tKdjviEMyt",
        "colab_type": "text"
      },
      "source": [
        "**Loss Function - Model Validation**   \n",
        "A loss function measures how far apart the current output of the model is from some target output. We'll use a common loss function, Sum of Squared Error (SSE).   \n",
        "This is calculated as:\n",
        "$\\text{Loss}=\\frac{1}{2}(y-\\hat y)^2$, where $y$ is the target and $\\hat y$ is our output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqoKVLzyFW7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e25b5a9d-e642-4a6f-8b0a-88cc4ff70c25"
      },
      "source": [
        "y = tf.placeholder(tf.float32) # Placeholder for target input.\n",
        "\n",
        "# Calculate SSE\n",
        "error = y - y_hat\n",
        "squared = tf.square(error)\n",
        "loss = tf.reduce_sum(squared) # Reduces dimension to 1 element.\n",
        "\n",
        "# Print loss\n",
        "print(sess.run(loss, {x: [1,2,3,4], y: [2,4,6,8]}))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-71QvMKHGxgH",
        "colab_type": "text"
      },
      "source": [
        "So we have quite a high loss, and we want to reduce that. We can use the ```tf.train``` module to optimize our model. The simplest optimizer is **gradient descent**, which uses the derivative of the loss w.r.t. each variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsCBMvmPHJLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "78dc5c08-757c-4451-f60a-1cbb67813efa"
      },
      "source": [
        "# Initialize our optimizer with a 0.01 learning rate.\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "records = [] # Record our loss over time.\n",
        "iterations = 20 # Train for this many iterations.\n",
        "\n",
        "for i in range(iterations):\n",
        "  sess.run(train, {x: [1,2,3,4], y: [2,4,6,8]})\n",
        "  records.append(sess.run(loss, {x: [1,2,3,4], y: [2,4,6,8]}))\n",
        "  \n",
        "print(sess.run([m,b]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([1.9654541], dtype=float32), array([0.10156948], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGg7CApOH1vN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "af28d8c6-487f-40ac-87de-f48f99e5d216"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(iterations), records, 'bo')\n",
        "plt.show()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvsbxM2bIkt-",
        "colab_type": "text"
      },
      "source": [
        "So there we go, our loss decreases over time and our model learns. On to the next tutorial.   \n",
        "*--Done 6/15/2019--*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm2Q-Uus5XiD",
        "colab_type": "text"
      },
      "source": [
        "# Back to the RL Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_57jhi3V0U4",
        "colab_type": "text"
      },
      "source": [
        "Now let's write the code for this problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl6Vwf8VzRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "bandits = [0.1,0.4,0.7,0.99]\n",
        "num_bandits = len(bandits)\n",
        "\n",
        "def pullBandit(bandit):\n",
        "  # Returns a good reward with odds of the given bandit.\n",
        "  r = random.random()\n",
        "  if r < bandit:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1FockqnXp31",
        "colab_type": "code",
        "outputId": "ea30e73e-c08d-42e0-c048-3d0693199b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "class PolicyGradient_ep:\n",
        "  \"\"\"A policy gradient agent that chooses epsilon-greedy actions.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_actions, ep):\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-6df895e34851>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqLVwRK636Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#These two lines established the feed-forward part of the network. This does the actual choosing.\n",
        "weights = tf.Variable(tf.ones([num_bandits]))\n",
        "chosen_action = tf.argmax(weights,0)\n",
        "\n",
        "#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
        "#to compute the loss, and use it to update the network.\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "responsible_weight = tf.slice(weights,action_holder,[1])\n",
        "loss = -(tf.log(responsible_weight)*reward_holder)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAm1PtM84AYi",
        "colab_type": "code",
        "outputId": "0c6a8642-eae0-4502-e36d-c7bfce1497dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
        "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
        "e = 0.01 #Set the chance of taking a random action.\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while i < total_episodes:\n",
        "        \n",
        "        #Choose either a random action or one from our network.\n",
        "        if np.random.rand(1) < e:\n",
        "            action = np.random.randint(num_bandits)\n",
        "        else:\n",
        "            action = sess.run(chosen_action)\n",
        "        \n",
        "        reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
        "        \n",
        "        #Update the network.\n",
        "        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
        "        \n",
        "        #Update our running tally of scores.\n",
        "        total_reward[action] += reward\n",
        "        if i % 50 == 0:\n",
        "            print(\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
        "        i+=1\n",
        "print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "if np.argmax(ww) == np.argmax(np.array(bandits)):\n",
        "    print(\"...and it was right!\")\n",
        "else:\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 bandits: [-1.  0.  0.  0.]\n",
            "Running reward for the 4 bandits: [-1. -2. -1. 43.]\n",
            "Running reward for the 4 bandits: [-1. -2. -1. 89.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 139.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 185.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 233.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 279.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.  -1. 329.]\n",
            "Running reward for the 4 bandits: [ -1.  -2.   0. 376.]\n",
            "Running reward for the 4 bandits: [ -1.  -1.   0. 421.]\n",
            "Running reward for the 4 bandits: [ -1.  -1.   0. 471.]\n",
            "Running reward for the 4 bandits: [ -1.   0.   0. 518.]\n",
            "Running reward for the 4 bandits: [ -1.   0.   0. 564.]\n",
            "Running reward for the 4 bandits: [ -2.   0.   0. 611.]\n",
            "Running reward for the 4 bandits: [ -3.   0.   0. 658.]\n",
            "Running reward for the 4 bandits: [ -3.   0.   0. 706.]\n",
            "Running reward for the 4 bandits: [ -3.   0.   0. 756.]\n",
            "Running reward for the 4 bandits: [ -4.   0.   0. 803.]\n",
            "Running reward for the 4 bandits: [ -4.   0.   0. 853.]\n",
            "Running reward for the 4 bandits: [ -4.  -1.  -1. 901.]\n",
            "The agent thinks bandit 4 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwum8PTA56NS",
        "colab_type": "text"
      },
      "source": [
        "The above code is from the RL tutorial; I need to learn pieces of TensorFlow before reading through this. Links to do so:\n",
        "https://appdividend.com/2019/02/06/tensorflow-variables-and-placeholders-tutorial-with-example/\n",
        "https://www.edureka.co/blog/tensorflow-tutorial/\n",
        "\n",
        "That's next time. Last revised 6/14/2019"
      ]
    }
  ]
}
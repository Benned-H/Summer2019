{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Summer2019/blob/master/Simple_RL_with_TF/Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3PuHWdvoI8Q",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 - Policy-based Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k8Kdm4hoMfz",
        "colab_type": "text"
      },
      "source": [
        "This time, we'll introduce a full reinforcement learning problem with observations, actions, and long-term rewards. Environments which pose full problems are referred to as Markov Decision Processes (MDPs). Such environments provide rewards and state transitions given actions, but the rewards also condition on the state and action taken. We could define a MDP as follows:   \n",
        "**Def**: A *Markov Decision Process* consists of a set of possible states $S$. At any time, an agent will experience some state $s\\in S$. In this state $s$, an agent could take any action $a$ from the set of possible actions $A$. Given a state-action pair $(s,a)$, the transition probability to some new state $s'$ is defined by $T(s,a)$, and the reward by $R(s,a)$. Thus at any time in a MDP, an agent is given some state $s\\in S$, takes action $a\\in A$, and receives a new state $s'$ and reward $r$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0qfDvj8rYrC",
        "colab_type": "text"
      },
      "source": [
        "**Cart-Pole Task** - We'll need a challenge a bit more complex than the two-armed bandit to have a full MDP. For this, we'll use the Cart-Pole environment provided by OpenAI Gym. This environment gives us:   \n",
        "* *Observations* - The agent receives where the cart is and what angle the pole is currently balanced at. With this information, our agent will learn to produce appropriate action probabilities.\n",
        "* *Delayed reward* - Our only goal is to keep the pole in the air as long as possible, so it's crucial that we consider how valuable our actions were in the long-term. We'll create a function to weight rewards over time by modifying our earlier Policy Gradient. To update our agent with more than one experience at a time, we'll collect experiences in a buffer and update the agent with an entire buffer at once. We call one of these sequences of experiences a rollout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt7qEXGUtaUd",
        "colab_type": "text"
      },
      "source": [
        "As a recap of TensorFlow object types, here's a summary of my notes from Part 1:\n",
        "* ```tf.constant(x)``` - Stores some constant value ```x``` which it will continually reproduce into the computational graph.\n",
        "* ```tf.Session()``` - Returns the Session which we'll use to run the computational graphs.\n",
        "* ```tf.placeholder(dtype, shape=None)``` - Allow the input of external parameters into the graph. Use this to input training examples or rollouts for RL. Must be fed inputs at runtime.\n",
        "* ```tf.Variable()``` - Allow the graph to give some different output w.r.t. the same input. These are the trainable parameters and are initialized with some value and optional datatype. These will survive across multiple executions of the graph.\n",
        "* ```sess.run(train, feed_dict={x: x_in, y: y_in})``` - Runs the graph ```train``` using Session ```sess```. We're also passing in inputs for placeholders ```x``` and ```y```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYvL9mMgxiPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d2eaa5f-ccdc-457a-fbcf-67b102a8376f"
      },
      "source": [
        "### Cart-Pole Vanilla Policy Gradient Agent ###\n",
        "\n",
        "import random\n",
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04887844,  0.04350822,  0.01944751, -0.01199158])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK2DfJ_ayC68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7eb3dea9-169a-479a-8b04-a867915391d0"
      },
      "source": [
        "# Survey the environment's observation and action spaces.\n",
        "print(env.observation_space) # \n",
        "print(env.action_space) # 0 or 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(4,)\n",
            "Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-uDnWUwywBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbe40b48-1934-4100-d08b-cc6fcedce26f"
      },
      "source": [
        "# This first reset just gives our starting state, or our\n",
        "# first set of observations.\n",
        "env.reset()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0421422 , 0.01901745, 0.00920982, 0.04300114])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVAptBIwxukI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount(arr, gamma):\n",
        "  \"\"\"Discounts a given list of rewards using the given gamma.\"\"\"\n",
        "  \n",
        "  for r in range(len(arr)): # Loop over all rewards\n",
        "    for i in range(r + 1, len(arr)): # For all future rewards...\n",
        "      arr[r] += gamma ** (i-r) * arr[i] # Add discounted future rewards\n",
        "    \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuCZueQbztKR",
        "colab_type": "text"
      },
      "source": [
        "I'll be reading through SpinningUp before continuing coding this, as the algorithmic foundations just aren't being taught in this series. Last revised 7/16/2019."
      ]
    }
  ]
}